%% LyX 2.3.3 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass[a4paper,english]{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{array}
\usepackage{float}
\usepackage{calc}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{stackrel}
\usepackage{graphicx}
\usepackage{wasysym}
\PassOptionsToPackage{normalem}{ulem}
\usepackage{ulem}

\makeatletter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% LyX specific LaTeX commands.
\pdfpageheight\paperheight
\pdfpagewidth\paperwidth

%% Because html converters don't know tabularnewline
\providecommand{\tabularnewline}{\\}
\floatstyle{ruled}
\newfloat{algorithm}{tbp}{loa}
\providecommand{\algorithmname}{Algorithm}
\floatname{algorithm}{\protect\algorithmname}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
\theoremstyle{definition}
    \ifx\thechapter\undefined
      \newtheorem{defn}{\protect\definitionname}
    \else
      \newtheorem{defn}{\protect\definitionname}[chapter]
    \fi
\theoremstyle{plain}
    \ifx\thechapter\undefined
	    \newtheorem{thm}{\protect\theoremname}
	  \else
      \newtheorem{thm}{\protect\theoremname}[chapter]
    \fi
\theoremstyle{remark}
    \ifx\thechapter\undefined
      \newtheorem{rem}{\protect\remarkname}
    \else
      \newtheorem{rem}{\protect\remarkname}[chapter]
    \fi
\theoremstyle{plain}
    \ifx\thechapter\undefined
  \newtheorem{cor}{\protect\corollaryname}
\else
      \newtheorem{cor}{\protect\corollaryname}[chapter]
    \fi
\theoremstyle{definition}
    \ifx\thechapter\undefined
      \newtheorem{example}{\protect\examplename}
    \else
      \newtheorem{example}{\protect\examplename}[chapter]
    \fi
\theoremstyle{plain}
    \ifx\thechapter\undefined
      \newtheorem{lem}{\protect\lemmaname}
    \else
      \newtheorem{lem}{\protect\lemmaname}[chapter]
    \fi

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usepackage{placeins}
\usepackage{graphicx}
\usepackage{textcomp}

\makeatother

\usepackage{babel}
\providecommand{\corollaryname}{Corollary}
\providecommand{\definitionname}{Definition}
\providecommand{\examplename}{Example}
\providecommand{\lemmaname}{Lemma}
\providecommand{\remarkname}{Remark}
\providecommand{\theoremname}{Theorem}

\begin{document}

\part{Preliminaries}

\section{Notation and Basic Terminology}

\paragraph{Vectors and Matrices}

Vectors $\boldsymbol{v}$ are denoted by underlined letters. Unless
stated otherwise, vectors are indexed starting from 1, i.e. $\boldsymbol{v}=\left[v_{1},\ldots,v_{n}\right]$.
Vectors are usually considered to be row vectors. Matrices $\boldsymbol{V}$
are shown in bold and capital letters. Elements of matrices or vectors
are surrounded by square brackets, and elements of tuples are surrounded
by round brackets.

\paragraph{Vector space}

A vector space of dimension $n$ over a finite field with $q$ elements
is denoted by $\ensuremath{\mathbb{F}}_{q}^{n}$. 

\paragraph{Linear Block Code}

>\textcompwordmark >\textcompwordmark > WHY ONLY LINEAR?

\textit{Block codes} are finite length codes first studied by Golay
\cite{Golay:1949} and Hamming \cite{Hamming:1950}. A \textit{linear}
$\left[n,k\right]_{q}$ \textit{block code} $\ensuremath{\mathcal{C}}$
of length $n$, dimension $k$ and codimension (or redundancy) $r=n-k$,
is a $k$-dimensional \textit{linear subspace} of $\ensuremath{\mathbb{F}}_{q}^{n}$.
A \textit{generator matrix} $\boldsymbol{G}$ has $k$ rows, which
form a \textit{basis} of $\ensuremath{\mathcal{C}}$, i.e., $\boldsymbol{G}$'s
row space is a set of $k$ linearly independent vectors generating
$\ensuremath{\mathcal{C}}$. The codeword $\underline{c}\in\ensuremath{\mathcal{C}}$
is encoded by the \textit{information vector} (or \textit{message})
$\boldsymbol{m}$, with $\boldsymbol{c}=\boldsymbol{m}\cdot\boldsymbol{G}$.
For any $\boldsymbol{c}\in\ensuremath{\mathcal{C}}$: $\boldsymbol{c}\cdot\boldsymbol{H}^{T}=\boldsymbol{0}$,
with $\boldsymbol{H}$ is a \textit{parity-check matrix} of $\ensuremath{\mathcal{C}}$,
whose row space generates the $\left[n,n-k\right]_{q}$ dual code
$\ensuremath{\mathcal{C}}^{\perp}$. $\left[n,k,d\right]_{q}$ is
equivalent to $\left[n,k\right]_{q}$ with minimum distance $d$ fulfills
$d=\underset{\boldsymbol{c}\in\ensuremath{\mathcal{C}\setminus\left\{ \boldsymbol{0}\right\} }}{min}d\left(\boldsymbol{c},\boldsymbol{0}\right)$
with respect to a metric $d\left(\cdot,\cdot\right):\ensuremath{\mathbb{F}}^{n}\times\ensuremath{\mathbb{F}}^{n}\rightarrow\mathbb{R}_{\geq0}$
on $\ensuremath{\mathbb{F}}^{n}$ \footnote{i.e., $d\left(\boldsymbol{x},\boldsymbol{y}\right)\geq0$,$d\left(\boldsymbol{x},\boldsymbol{y}\right)=0$
iff $\boldsymbol{x}=\boldsymbol{y}$, $d\left(\boldsymbol{x},\boldsymbol{y}\right)=d\left(\boldsymbol{y},\boldsymbol{x}\right)$
and $d\left(\boldsymbol{x},\boldsymbol{z}\right)\leq d\left(\boldsymbol{x},\boldsymbol{y}\right)+d\left(\boldsymbol{y},\boldsymbol{z}\right)$
for all $\boldsymbol{x},\boldsymbol{y},\boldsymbol{z}\in\ensuremath{\mathbb{F}}^{n}$.}.

\paragraph{Gaussian coefficient}

Gaussian coefficient (also known as $q$-binomial) counts the number
of subspaces of dimension $k$ in a vector space $\ensuremath{\mathbb{F}}_{q}^{n}$,

\[
\left[\begin{array}{c}
n\\
k
\end{array}\right]_{q}=\stackrel[i=0]{k-1}{\prod}\frac{q^{n}-q^{i}}{q^{k}-q^{i}}
\]


\paragraph{Multigraph}

A graph is permitted to have multiple edges. Edges that are incident
to same vertices can be in parallel. 

\paragraph{Directed Acyclic Graph}

A finite directed graph with no directed cycles, i.e. it consists
of a finite number vertices and edges, with each edge directed from
a vertex to another, such that there is no loop from any vertex $v$
with a sequence of directed edges back to the vertex again $v$.

\paragraph{Multicast}

Multicast communication supports the distribution of a data packet
to a group of users \cite{Zhang:2012}. It can be one-to-many or many-to-many
distribution \cite{Harte:2008}. In this study, we consider only one-to-many
multicast network.

\paragraph{Asymptotic Behavior}

For the combinatorial results, we study the asymtotic behaviour of
some formulas depending on the alphabet size $q$ and the vector length
$t$, by using the Bachmann-Landau notation, i.e. $\mathcal{O}\left(f\left(q,t\right)\right)$
for upper, $\Theta\left(f\left(q,t\right)\right)$ for tight, and
$\Omega\left(f\left(q,t\right)\right)$ for lower bounds, where $f$
is a function of the alphabet size and the vector length.

\section{Definition}

>\textcompwordmark >\textcompwordmark > EXPLAIN WHY I NEED EACH
DEFINITION?
\begin{defn}[Grassmannian Code]
 A Grassmannian code is a set of all subspaces of dimension $k\leq n$
in $\ensuremath{\mathbb{F}}_{q}^{n}$, and is denoted by $\mathcal{G}_{q}\left(n,k\right)$.
Due to being the set of all subspaces that have the same dimension
$k$, it is also called a \textit{constant dimension code}. \cite{Zhang:2019}
\end{defn}
%
\begin{defn}[Projective Space]
 The \textit{projective space of order} $n$ is a set of all subspaces
of $\ensuremath{\mathbb{F}}_{q}^{n}$, and is denoted by $\mathcal{P}_{q}\left(n\right)$,
i.e. a union of all dimension $k=0,\ldots n$ subspaces in $\ensuremath{\mathbb{F}}_{q}^{n}$
or $\mathcal{P}_{q}\left(n\right)=\bigcup_{k=0}^{n}\mathcal{G}_{q}\left(n,k\right)$.
\cite{Wachter-Zeh:2018}
\end{defn}
%
\begin{defn}[Covering Grassmannian code]
 An $\alpha-\left(n,k,\delta\right)_{q}^{c}$ covering Grassmannian
code (code in short) $\mathcal{C}$ is a subset of $\mathcal{G}_{q}\left(n,k\right)$
such that each subset of $\alpha$ codewords of $\mathcal{C}$ span
a subspace whose dimension is at least $\delta+k$ in $\ensuremath{\mathbb{F}}_{q}^{n}$.
\cite{Zhang:2019}
\end{defn}

\paragraph{The cardinality of a Grassmannian code}

The cardinality of $\mathcal{G}_{q}\left(n,k\right)$ is the Gaussian
coefficient (also known as $q$-binomial), which counts the number
of subspaces of dimension $k$ in a vector space $\ensuremath{\mathbb{F}}_{q}^{n}$,

\[
\left|\mathcal{G}_{q}\left(n,k\right)\right|=\left[\begin{array}{c}
n\\
k
\end{array}\right]_{q}=\stackrel[i=0]{k-1}{\prod}\frac{q^{n}-q^{i}}{q^{k}-q^{i}},
\]

where $q^{\left(n-k\right)k}\leq\left[\begin{array}{c}
n\\
k
\end{array}\right]_{q}\leq4q^{\left(n-k\right)k}$.

\begin{defn}[Subspace packing \cite{Etzion:2018}]
 A subspace packing $t-\left(n,k,\lambda\right)_{q}^{m}$ is a set
$\mathcal{S}$ of $k$-subspaces or $k$-dimensional subspaces (called
\textit{blocks}), such that each $t$-subspace of $\ensuremath{\mathbb{F}}_{q}^{n}$
is contained in at most $\lambda$ codewords of $\mathcal{C}$. 
\end{defn}
%
\begin{defn}[\cite{Etzion:2018}]
$\mathcal{A}_{q}\left(n,k,t;\lambda\right)$ denotes the maximum
size of a $t-\left(n,k,\lambda\right)_{q}^{m}$ code, where there
are no repeated codewords. 
\end{defn}

\section{Theorem}

>\textcompwordmark >\textcompwordmark > EXPLAIN BEFORE WHY I NEED
EACH THEOREM?
\begin{thm}[\cite{Zhang:2019}]
 If $n,k,t,$ and $\lambda$ are positive integers such that $1\leq t<k<n$
and $1\leq\lambda\leq\left[\begin{array}{c}
n-t\\
k-t
\end{array}\right]_{q}$, then

\[
\mathcal{A}_{q}\left(n,k,t;\lambda\right)\leq\left\lfloor \lambda\frac{\left[\begin{array}{c}
n\\
t
\end{array}\right]_{q}}{\left[\begin{array}{c}
k\\
t
\end{array}\right]_{q}}\right\rfloor 
\]
\end{thm}
%
\begin{thm}[\cite{Zhang:2019}]
 If $n,k,t,$ and $\lambda$ are positive integers such that $1\leq t<k<n$
and $1\leq\lambda\leq\left[\begin{array}{c}
n-t\\
k-t
\end{array}\right]_{q}$, then

\[
\mathcal{A}_{q}\left(n,k,t;\lambda\right)\leq\left\lfloor \frac{q^{n}-1}{q^{k}-1}\mathcal{A}_{q}\left(n-1,k-1,t-1;\lambda\right)\right\rfloor 
\]
\end{thm}

\part{Network Coding}

\section{What is Network Coding?}

Our considered communication network is a directed graph\footnote{Network coding over undirected networks was introduced in \cite{Li:2004}.}
allowing multiple links, i.e. edges, from one node to another. Each
\textit{link} in a network has \textit{unit capacity}, i.e. it carries
a packet which is either a symbol from $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}$
(in scalar network coding), or a vector of length $t$ over $\ensuremath{\mathbb{F}}_{q}$
(in vector network coding). Note that the assumption of unit capacity
does not restrict our considered networks, since links of larger capacity
can be represented by multiple parallel links of unit capacity. A
request of $h$ data units, i.e. $h$ packets or $h$ messages, is
therefore represented by a $h$-dimensional row vector $\boldsymbol{x}\in\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$
(in scalar network coding), or $\boldsymbol{x}\in\ensuremath{\mathbb{F}}_{q}^{th}$
(in vector network coding). In Figure \ref{fig:incoming_links}, a
node of a network is repsented with its \textit{incoming links} and
\textit{outgoing links}. A node without any incoming link is a \textit{source}
of the network. Packets are transmitted from the source to a set of
destination nodes, i.e. receivers, over error-free links, which is
still applicable to present-day wireline networks\footnote{Wireless network coding was introduced in \cite{Katti:2008}.}. 

\begin{figure}[H]
\caption{Incoming links and outgoing links of a node \label{fig:incoming_links}}

\centering{}\includegraphics[height=0.1\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/incoming_edges}
\end{figure}

In simple routing, information is transmitted from the source to each
destination node through a chain of \textit{intermediate nodes} by
a method known as store-and-forward \cite{Yeung:2006}. In this method,
the information can be represented as data packets, and the packets
received from an incoming link, i.e. inputs, of an intermediate node
can only be forwarded to a next node via an outgoing link as its output.
Network Coding was though first introduced in Ahlswede et al.'s seminal
paper as ``coding at a node in a network'', where coding means an
arbitrary combination of inputs for an output. It means that each
intermediate node in the network (not only at the source) is allowed
to forward a function of their received packets. A \textit{network
code} is a set of these functions of the packets on the links of the
network \cite{Wachter-Zeh:2018}. A network code is called a \textit{solution}
for the network, if each receiver can recover its requested packets
from the received packets on its incoming links. In other words, the
network is \textit{solvable}, if there exists an assignment of all
the functions on all the links of the network such that each receiver
can recover its requested information. If these functions are linear,
we obtain a \textit{linear network coding solution}, and we do not
consider \textit{nonlinear solution} thoughout this thesis. Each function
on a link consists of coding coefficients for each incoming packet.
The coding coefficients form a coefficient vector of length as its
number of incoming links, and this coefficient vector is called the
\textit{local coding vector}, which is distinguished with \textit{global
coding vector} defined in Section \ref{subsec:Matrix-channel}. If
the coding coefficients and the packets are scalars, a solution of
linear network coding is called \textit{scalar solution}. In \cite{Koetter:2003},
Kötter and M\'edard provided an algebraic formulation for the linear
network coding problem and its scalar solvability. 

\section{Advantages of Network Coding}

\paragraph{Throughput gain and reduced complexity}

Network coding gives a potential gain in throughput by communicating
more information with fewer packet transmissions compared to the routing
method. The butterfly network in \cite{Ahlswede:2000} as a multicast
in a wireline network is a standard example for an increase of throughput. 

\begin{figure}[H]
\caption{The butterfly network \label{fig:The-butterfly-network}}

\centering{}\includegraphics[width=0.5\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/ahlswede_butterfly_network}
\end{figure}

In Figure \ref{fig:The-butterfly-network}, we denoted a receiver
by ``Recv'', which is used for all of figures in this study. With
the help of network coding, both Recv 1 and 2 can recover $x_{1}$
and $x_{2}$ by a bitwise XOR in Figure \ref{fig:The-butterfly-network}(c).
Without network coding, an additional transmission between vertex
3 and 4 must be supplemented to communicate the contents of 2 packets
$x_{1}$ and $x_{2}$ from the source to Recv 1 and Recv 2, i.e. we
must communicate $x_{1}$ or $x_{2}$ separately on this link twice
under routing in Figure \ref{fig:The-butterfly-network}(a) and (b). 

\paragraph{Robustness and security}

\textit{Packet loss} is a particular issue in wireless packet networks
due to several reasons, e.g. buffer overflow or communication failures.
Sharing a common concept with Erasure Coding (EC) by exploiting a
degree of redundancy to packets on any vertices in the network, the
receivers are able to successfully recover the original packets from
a large number of packet losses, e.g. $101\circledast10\circledast1$.
The only difference is that packets are only encoded by the source
in EC \cite{Fujimura:2008}. This problem is dealed by acknowledgement
messages in the mechanism of transmission control protocol (TCP).

Network coding offers both benefits and drawbacks regarding to security.
For example, node 4 is operated by an eavesdropper and it obtains
only the packet $x_{1}\oplus x_{2}$, so it cannot obtain either $x_{1}$
or $x_{2}$ and the communication is secure. Alternatively, if the
eavesdropper controls node 3, it can anonymously send a fake packet
masquerading as $x_{1}\oplus x_{2}$, which is difficult to detect
in network coding \cite{Ho:2008}.

\paragraph{From scalar network coding to vector network coding}

Ebrahimi and Fragouli \cite{Ebrahimi:2011} have extended the algebraic
approach in \cite{Koetter:2003} to \textit{vector network coding}.
Here, all packets are vectors of length $t$, and the coding coefficients
are $\left[t\times t\right]$ matrices. The network code hence is
a set of functions consisting of $\left[t\times t\right]$ coding
matrices, and is called \textit{vector solution} if all receivers
can recover their requested information for such coding marices. In
\cite{Medard:2003}, an example of a network which does not have a
scalar solution, but is solvable by vector routing was shown. Although
it was shown that not every solvable network has a vector solution
in \cite[Lemma II.2]{Dougherty:2005}, Das and Rai proved in \cite{Das:2016}
that there exists a network with a vector solution of dimension $m$
but with no vector solution over any finite field whose the dimension
is less than $m$. When we refer the \textit{alphabet size} of a network
coding solution, we mean the field size $q_{\mathrm{s}}$ or $q_{v}$
of the finite field $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}$ or
$\ensuremath{\mathbb{F}}_{q_{v}}$ respectively for such a scalar
solution or a vector solution. The alphabet size is an important parameter
determining the amount of computation performed at each network vertice
\cite{Wachter-Zeh:2018}. The problem of finding the minimum required
alphabet size of a (linear or nonlinear) scalar network code for a
certain multicast network is NP-complete \cite{Langberg:2009,Lehman:2004}.
This thesis focus on determining the sovability of networks to measure
the gap, and our considered networks consist only error-free links,
we therefore do not consider error correction here. Furthermore, we
consider the solvability of networks by proving an existence of an
assignment for all functions or coding coefficients such that all
receiver can recover its requested information, so the function is
not arbitrary or random as mentioned in \cite{Ho:2003,Ahlswede:2000}.
The network structure is also known, i.e. our considered network is
coherent. We later distinguish scalar and vector network coding more
specifically in Section \ref{subsec:Matrix-channel},Section \ref{subsec:Scalar-network-coding},
and Section \ref{subsec:Vector-network-coding}.

\section{Network as a matrix channel \label{subsec:Matrix-channel}}

To formulate this description, the source has a set of disjoint messages
referred to packets which are either symbols from $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}$
(scalar coding) or vectors of length $t$ over $\ensuremath{\mathbb{F}}_{q}$
(vector coding). Each link in the network carries functions of the
packets, and a \textit{network code} is a set of these functions.
The network code is called \textit{linear} if all the functions are
linear and nonlinear otherwise. Each receiver $R_{j},j\in\left\{ 1,\ldots,N\right\} $
requests a subset of the source's length-$h$ messages, and this subset
is called a \textit{packet}. Through all the functions on the links
from the source to each receiver, the receiver obtains several linear
combinations of the $h$ messages to form a linear system of equations
for its requested packets. The coefficients of a linear combination
are called \textit{global coding vectors} \cite{Sanders:2003}. The
linear equation system that any receiver $R_{j}$ has to solve is
as following:

\begin{equation}
\begin{array}{c|c}
Scalar & Vector\\
\underset{\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{s}}{\underbrace{\left[\begin{array}{c}
y_{j_{1}}\\
\vdots\\
y_{j_{s}}
\end{array}\right]}}=\underset{\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{s\times h}}{\underbrace{\boldsymbol{A}_{j}}}\cdot\underset{\ensuremath{\mathbb{F}}_{q_{s}}^{h}}{\underbrace{\left[\begin{array}{c}
x_{1}\\
\vdots\\
x_{h}
\end{array}\right]}} & \underset{\ensuremath{\mathbb{F}}_{q}^{st}}{\underbrace{\left[\begin{array}{c}
\boldsymbol{y}_{j_{1}}\\
\vdots\\
\boldsymbol{y}_{j_{s}}
\end{array}\right]}}=\underset{\ensuremath{\mathbb{F}}_{q}^{st\times th}}{\underbrace{\boldsymbol{A}_{j}}}\cdot\underset{\ensuremath{\mathbb{F}}_{q}^{th}}{\underbrace{\left[\begin{array}{c}
\boldsymbol{x}_{1}\\
\vdots\\
\boldsymbol{x}_{h}
\end{array}\right]}}
\end{array}\label{eq:linear_system}
\end{equation}

The transfer matrix $\boldsymbol{A}_{j}$ contains the links' \textit{global
coding vectors}, which are combined by the coefficients of linear
combinations on $\alpha l$ links from $\alpha$ nodes and $\epsilon$
direct-links to the corresponding receiver $R_{j}$:

\[
\begin{array}{c|c}
Scalar & Vector\\
\boldsymbol{A}_{j}=\left[\begin{array}{c}
\boldsymbol{a}_{j_{1}}\\
\vdots\\
\boldsymbol{a}_{j_{\alpha\ell}}\\
\vdots\\
\boldsymbol{a}_{j_{\alpha\ell+\epsilon}}
\end{array}\right] & \boldsymbol{A}_{j}=\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\vdots\\
\boldsymbol{A}_{j_{\alpha l}}\\
\vdots\\
\boldsymbol{A}_{j_{\alpha l+\epsilon}}
\end{array}\right]
\end{array}
\]

In general, the network is represented as a matrix channel for both
scalar and vector coding:
\begin{defn}
Network As Matrix Channel

The channel output can be written as: $\boldsymbol{Y}_{j}=\boldsymbol{A}_{j}\cdot\boldsymbol{X}$
\end{defn}
Because we reconstruct $\boldsymbol{X}$ with knowing $\boldsymbol{A}_{j}$,
i.e. the network structure is known, our network is coherent. A network
is \textit{sovable} or a network code is a \textit{solution}, if each
receiver can reconstruct its requested messages or solve the system
with a unique solution for scalars $x_{1},\ldots,x_{h}$, or vectors
$\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{h}$. Therefore, we want
to find global coding vectors such that the matrix $\boldsymbol{A}_{j}$
has full-rank for every $j=1,\ldots,N$, and such that $q_{\mathrm{s}}$
or $q^{t}$ is minimized. In Example \ref{ex:scalar_vector_mapping},
we provide a vector solution of field size $q$ and dimension $t$,
which has the same alphabet size as a scalar solution of field size
$q^{t}$.

To summarize the notations of both scalar and vector coding, we represent
them as in Table~\ref{tab:notations}:

\begin{table}[H]
\caption{Notations of network coding}

\label{tab:notations} 
\centering{}%
\begin{tabular}{|>{\centering}p{0.2\paperwidth}|c|c|}
\hline 
 & Scalar Coding & Vector coding\tabularnewline
\hline 
\hline 
Source Messages/Packets & $\begin{array}{c}
x_{1},\ldots,x_{h}\in\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}\\
\boldsymbol{x}\in\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}
\end{array}$ & $\begin{array}{c}
\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{h}\in\ensuremath{\mathbb{F}}_{q}^{t}\\
\boldsymbol{x}\in\ensuremath{\mathbb{F}}_{q}^{th}
\end{array}$\tabularnewline
\hline 
Global Coding Vectors Of Receiver $R_{j}$ & $\boldsymbol{a}_{j_{1}},\ldots,\boldsymbol{a}_{j_{s}}\in\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$ & $\boldsymbol{A}_{j_{1}},\ldots,\boldsymbol{A}_{j_{s}}\in\ensuremath{\mathbb{F}}_{q}^{t\times th}$\tabularnewline
\hline 
Transfer Matrix Of Receiver $R_{j}$ & $\boldsymbol{A}_{j}\in\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{s\times h}$ & $\boldsymbol{A}_{j}\in\ensuremath{\mathbb{F}}_{q}^{st\times th}$\tabularnewline
\hline 
Packets On Receiver $R_{j}$ & $\begin{array}{c}
y_{j_{1}},\ldots,y_{j_{s}}\in\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}\\
\boldsymbol{y}\in\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{s}
\end{array}$ & $\begin{array}{c}
\boldsymbol{y}_{j_{1}},\ldots,\boldsymbol{y}_{j_{s}}\in\ensuremath{\mathbb{F}}_{q}^{t}\\
\boldsymbol{Y}_{j}\in\ensuremath{\mathbb{F}}_{q}^{st}
\end{array}$\tabularnewline
\hline 
Number of nodes & $r_{scalar}$ & $r_{vector}$\tabularnewline
\hline 
\end{tabular}
\end{table}

\begin{rem}
By using the vector coding, the upper bound number of solutions increases
from $q^{tkh}$ to $q^{t^{2}kh}$. Therefore, vector network coding
offers more freedom in choosing the coding coefficients than does
scalar linear coding for equivalent alphabet sizes, and a smaller
alphabet size might be achievable \cite{Ebrahimi:2011}. By this advantage,
we can have higher number of receivers, i.e. higher number of nodes,
in vector network coding.
\end{rem}

\section{Network Model}

\subsection{Multicast Networks as Generalized Combination Networks}

A class of networks which is mainly studied is the class of multicast
networks. It can be one-to-many or many-to-many distribution \cite{Harte:2008}.
In this study, we target one-to-many multicast network with the distribution
of a data packet to a group of users \cite{Zhang:2012}. An interesting
network structure often used for multicast networks in network coding
is called Combination Network (CN) and denoted by $\mathcal{N}_{h,r,s}$.
Many examples in previous studies demonstrating the advantage of network
coding have used structures identical or similar to that of CN. We
mention a few examples to emphasize CN's importance in the study of
network coding. In Figure \ref{fig:butterfly_nw_cn}, the butterfly
network that is often used as a first example to motivate network
coding, e.g. \cite[Fig. 7]{Ahlswede:2000} and \cite[Fig. 1]{Sanders:2003},
is isomorphic to $\mathcal{N}_{h,r=3,s=2}$, if we consider it as
an undirected network \cite{Maheshwar:2012}. The $\mathcal{N}_{h,r=3,s=2}$
itself was also used in the first study of network coding \cite{Ahlswede:2000}.
Other CNs, i.e. $\mathcal{N}_{h,r=4,s=2}$ and $\mathcal{N}_{h,r=6,s=3}$,
were also used as examples to demonstrate the advantage of network
coding in \cite[Fig. 2]{Sanders:2003} and \cite[Fig. 2]{Jaggi:2005}
respectively. The general structure of CN was also introduced and
discussed in \cite[Sec. 4.3]{Fragouli:2006}, \cite[Sec. 4.1]{Yeung:2006},
\cite{Ngai:2004,Xiao:2007}.
\begin{figure}[H]
\caption{The butterfly network is represented as a combination network \label{fig:butterfly_nw_cn}}

\centering{}\includegraphics[width=0.5\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/ahlswede_butterfly_network_CN}
\end{figure}

A generalization of a CN \cite{Riis:2006} is called generalized combination
network (GCN). GCN defined in \cite{Etzion:2016,Wachter-Zeh:2018}
was used to prove that vector network coding outperforms scalar linear
network coding , in multicast networks, with respect to the \textit{alphabet
size}, using rank-metric codes and Grassmannian codes. A comparison
between the required alphabet size for a scalar linear solution, a
vector solution, and a scalar nonlinear solution, of the same multicast
network is an important problem. Etzion and Wachter-Zeh introduced
a \textit{gap} in \cite{Etzion:2016} as the difference between the
smallest alphabet size for which a scalar linear solution exists and
the smallest alphabet size for which they can construct a vector solution.
They have found bounds on the gap for several network families of
GCN in \cite{Etzion:2016,Wachter-Zeh:2018}, but no gap for the GCN
networks with 3 messages has been found, i.e. $(1,1)-\mathcal{N}_{3,r,4}$,
where we denote GCN by $(\epsilon,l)-\mathcal{N}_{h,r,s}$. Therefore,
a combinatorial approach is first introduced in this thesis to prove
an existence of a vector solution outperforming the optimal scalar
linear solution with $q^{t^{2}/4+\mathcal{O}(t)}$. We then further
extend the approach for a family of GCN called One-Direct Link Combination
Network, i.e. $(1,1)-\mathcal{N}_{h,r,s}$. More formal definitions
of the gap and GCN can be found in Section \ref{sec:Description_GCN}. 

\subsection{Comparison between scalar and vector solutions by the gap size \label{subsec:Comparison-between-scalar-and-vector-sol}}

The \textit{gap} represents the difference between the smallest field
(alphabet) size for which a scalar linear solution exists and the
smallest alphatbet size for which we can construct a vector solution.
In this study, we define a solvable vector network coding over the
field size $\ensuremath{\mathbb{F}}_{q}^{t}$, and we find the minimum
amount of maximum nodes such vector solution can achieve, i.e. $r_{max,vector}\geq f_{1}(q,t)$,
with $f_{1}:\mathbb{Z}\mapsto\mathbb{Z}$. Meanwhile, we has a scalar
solution for the same network existing if and only if: $r_{scalar}\leq f_{2}\left(q_{\mathrm{s}}\right)$,
with $f_{2}:\mathbb{Z}\mapsto\mathbb{Z}$. To find the field size
$q_{\mathrm{s}}$ required for a scalar solution to reach the maximum
achievable vector solution's nodes in this setting, we consider $r_{max,scalar}=f_{2}\left(q_{\mathrm{s}}\right)=f_{1}(q,t)=r_{max,vector}$.
Finally, we calculate the gap by $g=q_{\mathrm{s}}-q_{v}=q_{\mathrm{s}}-q^{t}.$
Throughout this study, we show that vectors solutions significantly
reduce the required alphabet size by this gap. 

\part{Generalized Combination Network }

\section{Description \label{sec:Description_GCN}}

A generalized combination network $(\epsilon,\ell)-\mathcal{N}_{h,r,s}$
consists of 3 components over 3 layers from top to bottom: ``Source''
in the first layer, ``Intermediate Nodes'' in the middle layer,
and ``Receiver'' in the third layer. Because ``Source'' and ``Receiver''
have their own names without previous confusion of a source node or
a destination node, we replace ``Intermedate Nodes'' by ``Nodes''
from this section. The network has a source with $h$ messages, $r$
nodes, and $\left(\begin{array}{c}
r\\
\alpha
\end{array}\right)$ receivers, which form a single source multicast network modeled as
a finite directed acyclic multigraph \cite{Li:2003}. The source connects
to each node by $\ell$ parallel links and each node also connects
to a receiver by $\ell$ parallel links, which are respectively called
a node's incoming and outgoing edges. Each receiver is connected by
$s$ links in total, specifically $\alpha l$ links from $\alpha$
nodes and $\epsilon$ direct links from the source, i.e. $s=\alpha\ell+\epsilon$.
The combination network in \cite{Riis:2006} is the $(0,1)-\mathcal{N}_{h,r,s}$
network and the $(1,1)-\mathcal{N}_{h,r,s}$ network is called One-Direct
Link Combination Network. Theorem 1 shows our interest of relations
between the parameters $h,\alpha,\epsilon$ and $\ell$. Following
to Theorem \ref{nw_parameters}, we are interested in networks parameters
satisfying this condition: $\ell+\epsilon+1\leq h\leq\alpha\ell+\epsilon$.
\begin{figure}[H]
\caption{The generalized network $(\epsilon,\ell)-\mathcal{N}_{h,r,s}$\label{fig:The-generalized-network}}

\centering{}\includegraphics[width=0.5\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/generalized_combination_nw}
\end{figure}

\begin{thm}[\cite{Wachter-Zeh:2018}]
\label{nw_parameters}The $(\epsilon,\ell)-\mathcal{N}_{h,r,s}$
network has a trivial solution if $\ell+\epsilon\geq h$, and it has
no solution if $\alpha\ell+\epsilon<h$. 

Proof: Following to the network coding max-flow min-cut theorem for
multicast networks, the maximum number of messages from the source
to each receiver is equal to the smallest min-cut between the source
and any receiver. For our considered network, $s$ links have to be
deleted to disconnect the source from the receiver, which implies
that the min-cut between the source and each receiver is at least
$s$. Hence, $h\leq s\Leftrightarrow h\leq\alpha\ell+\epsilon$ $\Square$

There exist at least $\ell+\epsilon$ disjoint links connected to
each receiver. If $\ell+\epsilon\geq h$, each receiver can always
reconstruct its requested messages on its links. Then we only need
to do routing to select paths for the network. $\Square$
\end{thm}
\begin{table}[H]
\caption{Parameters of network coding \label{tab:Parameters-of-network}}

\centering{}%
\begin{tabular}{c|>{\centering}p{0.48\paperwidth}}
$h$ & The number of source messages\tabularnewline
\hline 
$r$ & The number of nodes in the middle layer\tabularnewline
\hline 
$\left(\begin{array}{c}
r\\
\alpha
\end{array}\right)$ & The number of receivers\tabularnewline
\hline 
$\ell$ & The source connects to each node by $\ell$ parallel links, and each
node also connects to one receiver by $\ell$ parallel links\tabularnewline
\hline 
$\alpha$ & A receiver is connected by any $\alpha$ nodes in the middle layer\tabularnewline
\hline 
$\epsilon$ & The source additionally connects to each receiver by $\epsilon$ direct
parallel links\tabularnewline
\hline 
$s$ & Each receiver is connected by $s$ links in total, with $s=\alpha\ell+\epsilon$.\tabularnewline
\end{tabular}
\end{table}


\section{Network Coding for This Network}

\subsection{Scalar network coding \label{subsec:Scalar-network-coding}}

A message is equivalent to a symbol over $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}$.
As a network of the multicast model, all receivers request the same
packet of $h$ symbols at the same time \cite{Trautmann:2013}. A
packet is a 1-dimentional subspace of $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$,
each receiver therefore must obtain a subspace of $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$,
whose dimension is at least $h$, to be able to reconstruct the packet.
Through $\epsilon$ direct links connected from the source to a receiver,
the source can provide any required $\epsilon$ 1-dimensional subspaces
of $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$ for the corresponding
receiver. Each receiver can accordingly reconstruct the packet if
and only if the linear span of $\alpha$ $\ell$-dimensional subspaces
of $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$ from the nodes
is at least of dimension $h-\epsilon$. When this necessary condition
is satisfied, the network is said to have a \textit{solution} or to
be \textit{solvable}.
\begin{thm}[\cite{Riis:2006}]
 The $(0,1)-\mathcal{N}_{h,r,s}$ network has a solution if and only
if there exists an $\left(r,\left|\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}\right|h,r-\alpha+1\right)$
$\left|\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}\right|$-ary error
correcting code. 
\end{thm}
%
\begin{thm}[\cite{Zhang:2019}]
 The $(\epsilon,\ell)-\mathcal{N}_{h,r,s=\alpha l+\epsilon}$network
is solvable over $\ensuremath{\mathbb{F}}_{q}$ if and only if there
exists an $\alpha-\left(h,\ell,h-\ell-\epsilon\right)_{q}^{c}$ code
with $r$ codewords. \label{theo:scalar_sol_exist}
\end{thm}

\subsection{Vector network coding \label{subsec:Vector-network-coding}}

The messages are vectors of length $t$ over $\ensuremath{\mathbb{F}}_{q}$,
i.e. a vector solution is over field size $q$ and dimension $t$.
Such a vector solution has the same alphabet size as a scalar solution
of field size $q^{t}$, and we denote $q_{v}=q^{t}$. A mapping from
the scalar solution of field size $q^{t}$ to a equivalent vector
solution is represented in Example \ref{ex:scalar_vector_mapping}.
Similarly with the scalar \textit{linear} coding solution, each receiver
can reconstruct its requested packet if and only if any $\alpha$
$\left(\ell t\right)$-dimensional subspaces span a subspace of dimension
at least $\left(h-\epsilon\right)t$.
\begin{thm}[\cite{Zhang:2019}]
 A vector solution for the $(\epsilon,\ell)-\mathcal{N}_{h,r,s}$
network exists if and only if there exists $\mathcal{G}_{q}\left(ht,\ell t\right)$
such that any $\alpha$ subspaces of the set span a subspace of dimension
at least $\left(h-\epsilon\right)t$. 
\end{thm}
%
\begin{thm}[\cite{Zhang:2019}]
 The $(\epsilon,\ell)-\mathcal{N}_{h,r,s=\alpha\ell+\epsilon}$network
is solvable with vectors of length $t$ over $\ensuremath{\mathbb{F}}_{q}$
if and only if there exists an $\alpha-\left(ht,\ell t,ht-\ell t-\epsilon t\right)_{q}^{c}$
code with $r$ codewords. 
\end{thm}
\begin{cor}
The $\alpha-\left(n=ht,n-k=ht-\ell t,\lambda=ht-\ell t-\epsilon t\right)_{q}^{m}$
code formed from the dual subspaces of the $\alpha-\left(n=ht,k=\ell t,\lambda=ht-\ell t-\epsilon t\right)_{q}^{c}$
code yields the upper bound of $\mathcal{A}_{q}\left(n=ht,n-k=ht-\ell t,\alpha;\lambda\right)$
as maximum number of nodes for a vector network coding of the $(\epsilon,\ell)-\mathcal{N}_{h,r,s}$
network. \label{cor:dual_subspaces}
\end{cor}
\begin{example}
\label{ex:scalar_vector_mapping} 

Given $h=3,q=2,t=2$, we consider the extension field $\ensuremath{\mathbb{F}}_{q^{t}=2^{2}}$.
This example shows how mapping messages from scalar coding to vector
coding.
\end{example}
\begin{figure}[H]
\caption{The mapping of scalar solution over $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}=q^{t}}$
to the equivalent vector solution\label{fig:x_mapping}}

\centering{}\includegraphics[width=0.3\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/x_mapping}
\end{figure}

We use the table of the extension field $\ensuremath{\mathbb{F}}_{2^{2}}$
with the primitive polynomial $f(x)=x^{2}+x+1$:
\begin{center}
\begin{tabular}{|c|c|c|}
\hline 
power of $\alpha$ & polynomial & binary vector\tabularnewline
\hline 
- & 0 & 00\tabularnewline
\hline 
$\alpha^{0}$ & 1 & 01\tabularnewline
\hline 
$\alpha^{1}$ & $\alpha$ & 10\tabularnewline
\hline 
$\alpha^{2}$ & $\alpha+1$ & 11\tabularnewline
\hline 
\end{tabular}
\par\end{center}

For scalar coding, the messages are $x_{1},\ldots,x_{h=3}\in\ensuremath{\mathbb{F}}_{2^{2}}$
, and for vector coding the messages are $\boldsymbol{x}_{1},\ldots,\boldsymbol{x}_{h=3}\in\ensuremath{\mathbb{F}}_{2}^{2}$.
From the polynomial column, let's choose arbitrarily a scalar vector
$\boldsymbol{x}_{scalar}=(x_{1},x_{2},x_{3})=(1,\alpha,\alpha+1)$.
Then, we map it to $\boldsymbol{x}_{vector}=(\boldsymbol{x}_{1},\boldsymbol{x}_{2},\boldsymbol{x}_{3})$
by using the binary vector column as following:

\[
\left[\begin{array}{c}
x_{1}=1\\
x_{2}=\alpha\\
x_{3}=\alpha+1
\end{array}\right]\mapsto\left[\begin{array}{c}
\left(\begin{array}{c}
1\\
0
\end{array}\right)\\
\left(\begin{array}{c}
0\\
1
\end{array}\right)\\
\left(\begin{array}{c}
1\\
1
\end{array}\right)
\end{array}\right],
\]

where we use the following rule for mapping $x_{i}$ individually:
$a_{0}\cdot\alpha^{0}+a_{1}\cdot\alpha^{1}+\ldots+a_{t-1}\cdot\alpha^{t-1}\mapsto\left(\begin{array}{c}
a_{0}\\
a_{1}\\
\vdots\\
a_{t-1}
\end{array}\right)$.

\section{Special Cases of Generalized Combination Network}

\subsection{The $(\ell-1)$-Direct Links and $\ell$-Parallel Links $\mathcal{N}_{h=2l,r,s=3l-1}$}

This family contains the largest number of direct links from the source
to the receivers. For $\ell\geq2$, this network $\left(\epsilon=\ell-1,\ell\right)-\mathcal{N}_{h=2\ell,r,s=3\ell-1}$
yields the gap $q^{(\ell-1)t^{2}/\ell+\mathcal{O}(t)}$ between vector
solutions and optimal scalar solutions. The vector solution is based
on an $\mathcal{MRD}\left[\ell t\times\ell t,t\right]_{q}$ code.
Further, the gap tends to $q^{t^{2}/2+\mathcal{O}(t)}$ for large
$\ell$.
\begin{lem}[\cite{Wachter-Zeh:2018}]
 There is a scalar linear solution of field size $q_{\mathrm{s}}$
for the $\left(\epsilon=\ell-1,\ell\right)-\mathcal{N}_{h=2\ell r,s=3\ell-1}$
network, where $\ell\geq2$, if and only if $r\leq\left[\begin{array}{c}
2\ell\\
\ell
\end{array}\right]_{q_{\mathrm{s}}}$.
\end{lem}
\begin{figure}[H]
\caption{The $\left(1,2\right)-\mathcal{N}_{4,r,5}$ network as an example
of the $\left(\ell-1,\ell\right)-\mathcal{N}_{2\ell,r,3\ell-1}$ \label{fig:network_l1e2h4rs5}}

\centering{}\includegraphics[width=0.4\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/nw_e1_l2_h4_r_s5}
\end{figure}


\subsection{The 1-Direct Link and $\ell$-Parallel Links $\mathcal{N}_{h=2\ell,r,s=2\ell+1}$}

This is the smallest direct-link family has an vector solution outperforming
the optimal scalar solution, i.e. an vector solution outperforming
the optimal scalar has not yet been found for the network $(0,\ell>1)-\mathcal{N}_{h,r,s}$.
Similar to the previous subfamily $\left(\epsilon=\ell-1,\ell\right)-\mathcal{N}_{h=2\ell,r,s=3\ell-1}$,
when $\ell\geq2$ or $h\geq4$, this network yields the largest gap
$q^{t^{2}/2+\mathcal{O}(t)}$ in the alphabet size by using the same
approach with an $\mathcal{MRD}\left[\ell t\times\ell t,(\ell-1)t\right]_{q}$
code. 

\begin{figure}[H]
\caption{The $\left(\ell-1,\ell\right)-\mathcal{N}_{2\ell,r,3\ell-1}$ network
\label{fig:network_special2}}

\centering{}\includegraphics[width=0.45\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/nw_special2}
\end{figure}


\subsection{The $\epsilon$-Direct Links $\mathcal{N}_{h,r,s}$}

This family is denoted as $\left(\epsilon\geq1,\ell=1\right)-\mathcal{N}_{h,r,s}$
and is the main focus of this thesis, because it motivates some interesting
questions on a classic coding problem and on a new type of subspace
code problem. In Section \ref{sec:Network_e1l1h3rs4}, we show our
largest code set with low number of subspace codes for the network
$\left(\epsilon=1,\ell=1\right)-\mathcal{N}_{h=3,r,s=4}$. Furthermore,
there is no gap size is known for this network in previous studies.

\begin{figure}[H]
\caption{The $\left(\epsilon\protect\geq1,\ell=1\right)-\mathcal{N}_{h,r,s}$
network \label{fig:network_special3}}

\centering{}\includegraphics[width=0.45\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/nw_special3}
\end{figure}


\subsection{The $\left(\epsilon=0,\ell=1\right)-\mathcal{N}_{h,r,s}$ Combination
Network}

Since the scalar solution for the combination network uses an $MDS$
code, a vector solution based on subspace codes must go beyond the
$MDS$ bound, i.e. Singleton bound $d\leq n-k+1$, to outperform the
scalar one. In paper \cite{Wachter-Zeh:2018}, it is proved that vector
solutions based on subspace codes cannot outperform optimal scalar
linear solutions for $h=2$, and they conjecture it for all $h$.
Unfortunately, a vector solution based on an $\mathcal{MRD}\left[t\times t,t\right]_{q}$
code is also proved that it cannot outperform the optimal scalar linear
solution.

\begin{figure}[H]
\caption{The $\left(\epsilon=0,\ell=1\right)-\mathcal{N}_{h,r,s}$ combination
network \label{fig:network_special4}}

\centering{}\includegraphics[width=0.4\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/nw_special4_combination}
\end{figure}


\subsection{The Largest Possible Gap between $q_{v}$ and $q_{s}$ in Previous
Studies}

\subsubsection{$h\protect\leq2\ell$ and $\epsilon\protect\neq0$}

For this network, the number of direct links is at least 1, i.e. $\epsilon\geq1$,
and the number of parallel links is less than half of the number of
source messages, i.e. $\ell\leq\frac{h}{2}$.

\paragraph{h is even}

The above $\left(\ell-1,l\right)-\mathcal{N}_{2\ell,r,3\ell-1}$ network
achieves the largest gap $q_{\mathrm{s}}=q^{(h-2)t^{2}/h+\mathcal{O}(t)}$.

\paragraph{h is odd}

The $\left(\ell-2,\ell\right)-\mathcal{N}_{2\ell-1,r,3\ell-2}$ network
achieves the largest gap $q_{\mathrm{s}}=q^{\left(h-3\right)t^{2}/\left(h-1\right)+\mathcal{O}(t)}$

\subsubsection{$h\protect\geq2\ell$ and $\epsilon\protect\neq h-2\ell$}

\paragraph{h is even}

The same above $\left(\ell-1,\ell\right)-\mathcal{N}_{2\ell,r,3\ell-1}$
network achieves the largest gap $q_{s}=q^{(h-2)t^{2}/h+\mathcal{O}(t)}$.

\paragraph{h is odd}

The $\left(\ell-1,\ell\right)-\mathcal{N}_{2\ell+1,r,3\ell-1}$ network
achieves the largest gap $q_{\mathrm{s}}=q^{(h-3)t^{2}/\left(h-1\right)+\mathcal{O}(t)}$.
\begin{rem}
The achieved gap is $q^{(h-2)t^{2}/h+\mathcal{O}(t)}$ for any $q\geq2$
and any even $h\geq4$. If $h\geq5$ is odd, then the achieved gap
of the alphabet size is $q^{(h-3)t^{2}/\left(h-1\right)+\mathcal{O}(t)}$
\cite{Wachter-Zeh:2018}.
\end{rem}

\part{Combinatorial Results}

In previous studies \cite{Wachter-Zeh:2018}, there was no general
vector solution found for multicast networks with $h=3$ messages.
Hence, we start with a probabilistic argument to prove that there
exists a vector solution outperforming the optimal linear solution
for the $\left(\epsilon=1,\ell=1\right)-\mathcal{N}_{h=3,r,s=4}$
network. Then we generalize the proof to the $\left(\epsilon=1,\ell=1\right)-\mathcal{N}_{h,r,s}$
network.

\section{$\left(\epsilon=1,\ell=1\right)-\mathcal{N}_{h=3,r,s=4}$ Network
\label{sec:Network_e1l1h3rs4}}

\begin{figure}[H]
\caption{The $(\epsilon=1,\ell=1)-\mathcal{N}_{h=3,r,s=4}$ network\label{fig:nw_e1_l1_h3_r_s4}}

\centering{}\includegraphics[width=0.5\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/nw_e1_l1_h3_r_s4}
\end{figure}

In this subsection, we derive a lower bound on the number of receivers
for the $\left(\epsilon=1,\ell=1\right)-\mathcal{N}_{h=3,r,s=4}$
network. Due to $\alpha=3$, the number of receivers is $N=\left(\begin{array}{c}
r_{vector}\\
3
\end{array}\right)$ by definition in Section \ref{sec:Description_GCN}. To derive the
lower bound, we introduce a rank requirement on incoming packets to
each receiver. 

\begin{figure}[H]
\caption{The vector network coding of $(\epsilon=1,l=1)-\mathcal{N}_{h=3,r,s=4}$
represents as a matrix problem\label{fig:rk_h3}}

\centering{}\includegraphics[width=0.6\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/rk_h3}
\end{figure}

Following to Equation \ref{eq:linear_system}, each receiver must
solve a linear equation system of 3 variables with 4 equations to
recover $h=3$ messages as below:

\[
\left[\begin{array}{c}
\boldsymbol{y}_{j_{1}}\\
\boldsymbol{y}_{j_{2}}\\
\boldsymbol{y}_{j_{3}}\\
\boldsymbol{y}_{j_{4}}
\end{array}\right]=\boldsymbol{A}_{j}\cdot\underline{x}=\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}\\
\boldsymbol{A}_{j_{4}}
\end{array}\right]\cdot\left[\begin{array}{c}
\boldsymbol{x}_{1}\\
\boldsymbol{x}_{2}\\
\boldsymbol{x}_{3}
\end{array}\right],
\]

with $\boldsymbol{x}_{i},\boldsymbol{y}_{j_{v}}\in\ensuremath{\mathbb{F}}_{q}^{t},\boldsymbol{A}_{j_{v}}\in\ensuremath{\mathbb{F}}_{q}^{t\times3t}$
for $v=1,\ldots,4$, and $\boldsymbol{A}_{j_{1}},\ldots,\boldsymbol{A}_{j_{3}}$
must be distinct.

The network is solvable, if $\boldsymbol{A}_{j}$ has full-rank, i.e.
$\boldsymbol{A}_{j_{v}}$ must satisfy:

\[
rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}\\
\boldsymbol{A}_{j_{4}}
\end{array}\right]\geq3t
\]

In order to satisfy $rk\left[\boldsymbol{A}_{j}\right]\geq3t$, we
can easily choose suitable values for the coefficient $\boldsymbol{A}_{j_{4}}$
on the direct link from the source to $R_{j}$. However, the coefficients
on the links from nodes to receivers are matters. Therefore, we focus
on the following requirement:

\begin{equation}
rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}
\end{array}\right]\geq2t\label{eq:rk_rqm_e1l1h3s4}
\end{equation}

This means that $rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}
\end{array}\right]\geq2t$ implies $rk\left[\boldsymbol{A}_{j}\right]\geq3t$, or to satisfy
$rk\left[\boldsymbol{A}_{j}\right]\geq3t$, we need $rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}
\end{array}\right]\geq2t$.

By this constraint, the problem is thus described as below:

\[
\underset{rk\left[\boldsymbol{A}_{j}\right]\geq3t}{min}\,r_{vector}
\]

$\boldsymbol{A}_{j_{1}},\boldsymbol{A}_{j_{2}},\boldsymbol{A}_{j_{3}}$
are matrices formed on any 3 of $r$ links from nodes to receivers,
i.e. these 3 matrices are randomly chosen from a set of $r$ matrices.
We formalize the problem by an approach with Lov\'asz local lemma
\cite{MosheSchwartz:2018}.
\begin{lem}[Symmetric Lov\'asz local lemma (LLL) \cite{Schwarz:2013}]
 A set of events $\mathcal{E}_{i}$, with $i=1,\ldots,n$, such that
each event occurs with probability at most $p$. If each event is
independent of all others except for at most $d$ of them and $4dp\leq1$,
then: $Pr\left[\stackrel[i=1]{n}{\bigcap}\overline{\mathcal{E}}_{i}\right]>0$.
\label{thm:LLL}
\end{lem}
%
\begin{lem}
For the network $\left(\epsilon=1,l=1\right)-\mathcal{N}_{h=3,r,s=4}$,
the probability that vector solution does not exist is equal to $\frac{1}{q^{9t^{2}}}\stackrel[i=0]{2t-1}{\mathop{\sum}}\stackrel[j=0]{i-1}{\mathop{\prod}}\frac{\left(q^{3t}-q^{j}\right)^{2}}{q^{i}-q^{j}}$.
\label{lem:prob_p_LLL_formula}
\end{lem}
\textit{Proof}. For our problem, let $\mathcal{E}_{i}$ denote the
following event:

\[
Pr\left[\mathcal{E}_{i}\right]=Pr\left[rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}
\end{array}\right]<2t\right]
\]

Because the rank requirement in Equation \ref{eq:rk_rqm_e1l1h3s4}
is opposite, we consider the complement event $T$:

\[
rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}
\end{array}\right]\geq2t,\forall1\leq j_{1}<j_{2}<j_{3}\leq r
\]

By the intersection rule, we have:

\[
T=\underset{\mathcal{E}_{i}\in\mathcal{E}}{\bigcap}\overline{\mathcal{E}}_{i}
\]

The probability of event $T$ indicates a measure quantifying the
likelihood that we will be able to construct $rk\left[\boldsymbol{A}_{j}\right]\geq3t$
with $j_{1,}j_{2},j_{3}$ in the integer numbers between $1$ and
$r$, including both. We need to maximize $r$, and the rank requirement
\ref{eq:rk_rqm_e1l1h3s4} must be satisfied, i.e. the probabilty of
event $T$ must be higher than $0$:

\begin{eqnarray*}
 & Pr\left[rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\boldsymbol{A}_{j_{2}}\\
\boldsymbol{A}_{j_{3}}
\end{array}\right]\geq2t,\forall1\leq j_{1}<j_{2}<j_{3}\leq r\right] & >0\\
\Leftrightarrow & Pr\left[T\right] & >0\\
\Leftrightarrow & Pr\left[\underset{\mathcal{E}_{i}\in\mathcal{E}}{\bigcap}\overline{\mathcal{E}}_{i}\right] & >0
\end{eqnarray*}

Following to LLL, each event occurst with probability at most $p$:

\begin{equation}
Pr\left[\mathcal{E}_{i}\right]=Pr\left[rk\left[\begin{array}{c}
\boldsymbol{A}_{i_{1}}\\
\boldsymbol{A}_{i_{2}}\\
\boldsymbol{A}_{i_{3}}
\end{array}\right]<2t\right]\leq p\label{eq:p_in_LLL}
\end{equation}

Regarding to the left-hand side:

\begin{eqnarray}
Pr\left[rk\left[\begin{array}{c}
\boldsymbol{A}_{i_{1}}\\
\boldsymbol{A}_{i_{2}}\\
\boldsymbol{A}_{i_{3}}
\end{array}\right]<2t\right] & = & \stackrel[i=0]{2t-1}{\mathop{\sum}}Pr\left[rk\left[\begin{array}{c}
\boldsymbol{A}_{i_{1}}\\
\boldsymbol{A}_{i_{2}}\\
\boldsymbol{A}_{i_{3}}
\end{array}\right]=i\right]\nonumber \\
 & \overset{1}{=} & \stackrel[i=0]{2t-1}{\mathop{\sum}}\frac{N_{t,m,n}}{q^{m\cdot n}}\nonumber \\
 & = & \stackrel[i=0]{2t-1}{\mathop{\sum}}\frac{\stackrel[j=0]{i-1}{\mathop{\prod}}\frac{\left(q^{m}-q^{j}\right)\left(q^{n}-q^{j}\right)}{q^{i}-q^{j}}}{q^{m\cdot n}}\nonumber \\
 & \overset{2}{=} & \stackrel[i=0]{2t-1}{\mathop{\sum}}\frac{\stackrel[j=0]{i-1}{\mathop{\prod}}\frac{\left(q^{3t}-q^{j}\right)^{2}}{q^{i}-q^{j}}}{q^{9t^{2}}}\Square\label{eq:p_eq_h3}
\end{eqnarray}

By varying $t$ in Equation (\ref{eq:p_eq_h3}), we have the following
table:

\begin{table}[H]
\caption{$r$ over variations of t\label{tab:r_over_t}}

\begin{centering}
\begin{tabular}{|c|c|c|}
\hline 
t & Scalar Solution & Vector Solution\tabularnewline
\hline 
\hline 
1 & $r_{scalar}\leq14$ & $r_{vector}\geq3$\tabularnewline
\hline 
2 & $r_{scalar}\leq42$ & $r_{vector}\geq7\,\left(67^{*},\,89^{**}\right)$\tabularnewline
\hline 
3 & $r_{scalar}\leq146$ & $r_{vector}\geq62\,\left(166^{*}\right)$ \tabularnewline
\hline 
4 & $r_{scalar}\leq546$ & $r_{vector}\geq1317$\tabularnewline
\hline 
5 & $r_{scalar}\leq2114$ & $r_{vector}\geq58472$\tabularnewline
\hline 
6 & $r_{scalar}\leq8322$ & $r_{vector}>10^{6}$\tabularnewline
\hline 
\end{tabular}
\par\end{centering}
{*}, {*}{*}: computational results in construction 1 and construction
2 respectively
\end{table}

In the table (\ref{tab:r_over_t}), the vector solution outperforms
the scalar solution when $t\geq4$ for the network $\left(\epsilon=1,l=1\right)-\ensuremath{N}_{h=3,r,s=4}$.
This is sufficient, later on we show computational results which vector
solutions outperform scalar solutions in case of $t=2$ and $t=3$.
\begin{lem}
For the network $\left(\epsilon=1,l=1\right)-\mathcal{N}_{h=3,r,s=4}$,
the probability that vector solution does not exist is less than or
equal to $\Theta\left(q^{-t^{2}-2t-1}\right)$. \label{lem:tight_bound_p}
\end{lem}
\textit{Proof}. This lemma is a tight bound for Equation \ref{eq:p_in_LLL}
in Lemma \ref{lem:prob_p_LLL_formula}, i.e. we try to maximize $p$
with an exact maximum value. We consider the nominator of Equation
(\ref{eq:p_eq_h3}):

\[
\stackrel[j=0]{i-1}{\mathop{\prod}}\frac{\left(q^{3t}-q^{j}\right)^{2}}{q^{i}-q^{j}}=\frac{p_{N}^{(i)}(q)}{p_{D}^{(i)}(q)}=p^{(i)}(q)
\]

Due to $i$-times product and large $t$:

\[
\left.\begin{array}{c}
deg\left(p_{N}^{(i)}(q)\right)=q^{i6t}\\
deg\left(p_{D}^{(i)}(q)\right)=q^{i^{2}}
\end{array}\right\} \Rightarrow p^{(i)}(q)\approx q^{i6t-i^{2}}
\]

Then we have:

\[
\stackrel[i=0]{2t-1}{\mathop{\sum}}\stackrel[j=0]{i-1}{\mathop{\prod}}\frac{\left(q^{3t}-q^{j}\right)^{2}}{q^{i}-q^{j}}=\stackrel[i=0]{2t-1}{\mathop{\sum}}p^{(i)}(q)\approx\stackrel[i=0]{2t-1}{\mathop{\sum}}q^{i6t-i^{2}}
\]

To maximize the sum, we set derivation of to 0 and find its root:

\begin{eqnarray*}
 & \left(i6t-i^{2}\right)^{'} & =0\\
\Leftrightarrow & 6t-2i & =0\\
\Leftrightarrow & i & =3t
\end{eqnarray*}

However, the upper limit of sum is $\left(2t-1\right)$, which is
less than $3t$.

\[
\Rightarrow max\left\{ q^{i6t-i^{2}}:i=0,2\ldots,2t-1\right\} =\left.q^{i6t-i^{2}}\right|_{i=2t-1}=q^{8t^{2}-2t-1}
\]

Hence, by using the exact bound $\Theta$, we have:

\[
\stackrel[i=0]{2t-1}{\mathop{\sum}}p^{(i)}(q)\in\Theta\left(max\left\{ q^{i6t-i^{2}}:i=1,2\ldots,2t-1\right\} \right)=\Theta\left(q^{8t^{2}-2t-1}\right)
\]

\[
\Rightarrow\frac{\stackrel[i=0]{2t-1}{\mathop{\sum}}p^{(i)}(q)}{q^{9t^{2}}}\in\Theta\left(q^{-t^{2}-2t-1}\right)\Square
\]

\begin{lem}
If $d\leq\frac{3}{2}r^{2}$, we have $r_{max,vector}\geq\Omega\left(q^{t^{2}/2+\mathcal{O}\left(t\right)}\right)$.
\label{lem:lower_bound_r_max_vector}
\end{lem}
\textit{Proof}. We proceed the other constraint of LLL in Lemma \ref{thm:LLL}:
$4dp\leq1$. Regarding to $d$, we have:

\begin{eqnarray*}
d(r) & \leq & 3\cdot\left(\begin{array}{c}
r-1\\
2
\end{array}\right)=3\cdot\frac{\left(r-1\right)\left(r-2\right)}{2}=\frac{3}{2}\left(r^{2}-3r+2\right)\\
 & \leq & \frac{3}{2}r^{2}=d_{max}(r)
\end{eqnarray*}

Because $4pd_{max}(r)\leq1$ implies that $4pd\leq1$, we consider
$d_{max}(r)$ directly:

\[
4\cdot p\cdot d_{max}(r)\leq1\Rightarrow4\cdot p\cdot\frac{3}{2}r^{2}\leq1\Rightarrow r\leq\sqrt{\frac{1}{6p}}=r_{max,vector}
\]

Similarly with above, $d$ and $r$ are propotional, so minimizing
$r$ is equivelent to maximizing $p$. The purpose is to achieve a
strict lower bound proving vector solutions always outperform scalar
solutions in a specific range of $t$, i.e., $r_{max,vector}$ asymptotes
to a value higher than $r_{max,scalar}$.

By applying Lemma \ref{lem:tight_bound_p}, we have:

\[
r_{max,vector}\in\Omega\left(\sqrt{\frac{1}{6p}}\right)=\Omega\left(\sqrt{\frac{1}{6q^{-t^{2}-2t-1}}}\right)=\Omega\left(q^{t^{2}/2+\mathcal{O}\left(t\right)}\right)\Square
\]

\begin{thm}
For the network $\left(\epsilon=1,l=1\right)-\mathcal{N}_{h=3,r,s=4}$,
the achieved gap is $q^{t^{2}/4+\mathcal{O}(t)}$.
\end{thm}
\textit{Proof}. In advance, we have: $r_{max,scalar}\in\mathcal{O}\left(q_{\mathrm{s}}^{2}\right)$
\cite{Wachter-Zeh:2018}, specifically, 
\begin{equation}
r_{scalar}\leq2\left[\begin{array}{c}
3\\
1
\end{array}\right]_{q_{\mathrm{s}}}=2\left(q_{\mathrm{s}}^{2}+q_{\mathrm{s}}+1\right)\label{eq:r_scalar_max}
\end{equation}
. 

Finally, following to Section \ref{subsec:Comparison-between-scalar-and-vector-sol}
and Lemma \ref{lem:lower_bound_r_max_vector}, we have the gap size:

\begin{eqnarray}
 & r_{max,scalar} & =r_{max,vector}\nonumber \\
\Leftrightarrow & q_{\mathrm{s}}^{2} & =q^{t^{2}/2+\mathcal{O}(t)}\nonumber \\
\Leftrightarrow & q_{\mathrm{s}} & ^{=}q^{t^{2}/4+\mathcal{O}(t)}\nonumber \\
\Rightarrow & g & =q_{\mathrm{s}}-q_{v}=q^{t^{2}/4+\mathcal{O}(t)}\Square\label{eq:gap_e1l1h3rs4}
\end{eqnarray}


\section{$\left(\epsilon=1,l=1\right)-\mathcal{N}_{h,r,s}$ Network}

\subsection{Find the lower bound of $r_{max,vector}$}

Following to Theorem (\ref{nw_parameters}), we are interested in
the following range: $\ell+\epsilon+1\leq h\leq\alpha\ell+\epsilon$.

As previous, $\boldsymbol{A}_{j_{1}},\ldots,\boldsymbol{A}_{j_{h-\epsilon}}\in\ensuremath{\mathbb{F}}_{q}^{t\times ht}$
and we need to satisfy the following:

\[
rk\left[\begin{array}{c}
\boldsymbol{A}_{j_{1}}\\
\vdots\\
\boldsymbol{A}_{j_{h-\epsilon}}
\end{array}\right]\geq ht-t\Leftrightarrow rk\left[\boldsymbol{A}_{j}\right]\geq(h-1)t
\]

We can formulate it by the following coding problem in Grassmannian:

\noindent\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
Find the largest set of subspaces from $\mathcal{G}_{q}\left(ht,t\right)$
such that any $\alpha$ subspaces of the set span a subspace of dimension
at least $\left(h-1\right)t$.%
\end{minipage}}

Similar to $\left(\epsilon=1,\ell=1\right)-\ensuremath{N}_{3,r,4}$,
we consider $p$ to proceed LLL:

\[
Pr\left[rk\left[\boldsymbol{A}\right]<(h-1)t\right]\leq p
\]

Regarding to the left-hand side:

\begin{eqnarray}
Pr\left[rk\left[\boldsymbol{A}\right]<(h-1)t\right] & = & \stackrel[i=0]{(h-1)t-1}{\mathop{\sum}}Pr\left[rk\left[\boldsymbol{A}\right]=i\right]\nonumber \\
 & \overset{1}{=} & \stackrel[i=0]{(h-1)t-1}{\mathop{\sum}}\frac{N_{i,\alpha t,ht}}{q^{\left(\alpha t\right)\left(ht\right)}}\nonumber \\
 & = & \frac{1}{q^{\left(\alpha h\right)t^{2}}}\cdot\stackrel[i=0]{(h-1)t-1}{\mathop{\sum}}\stackrel[j=0]{i-1}{\mathop{\prod}}\frac{\left(q^{\alpha t}-q^{j}\right)\left(q^{ht}-q^{j}\right)}{q^{i}-q^{j}}\label{eq:general_nw_calc_p}
\end{eqnarray}

Firstly, we consider the product:

\[
\stackrel[j=0]{i-1}{\mathop{\prod}}\frac{\left(q^{\alpha t}-q^{j}\right)\left(q^{ht}-q^{j}\right)}{q^{i}-q^{j}}=\frac{p_{N}^{(i)}(q)}{p_{D}^{(i)}(q)}=p^{(i)}(q)
\]

For $t\rightarrow\infty$:

\[
\left.\begin{array}{c}
deg\left(p_{N}^{(i)}(q)\right)=q^{i(\alpha t+ht)}\\
deg\left(p_{D}^{(i)}(q)\right)=q^{i^{2}}
\end{array}\right\} \Rightarrow p^{(i)}(q)\approx q^{i(\alpha t+ht)-i^{2}}
\]

Now, we evaluate $f(i)=i(\alpha t+ht)-i^{2}$ to find its maximum
point:

$\dot{f}(i^{*})=0\Leftrightarrow(\alpha t+ht)-2i^{*}=0\Leftrightarrow i^{*}=\frac{\alpha t+ht}{2}$

We then check whether this point within the range $i=0,\ldots,(h-1)t-1$
as following: $0\leq\frac{\alpha t+ht}{2}\leq(h-1)t-1$

With regards to the lower bound: $0\leq\frac{\alpha t+ht}{2}\Leftrightarrow t\geq\frac{2}{\alpha+h}$,
which is always true due to the given $t\geq2$ and $\alpha,h\geq3$.

Regarding to the upper bound: $\frac{\alpha t+ht}{2}\leq(h-1)t-1\Leftrightarrow t\leq\frac{-2}{\alpha+2-h}$
with $\alpha+2>h$ due to the given $\alpha l+\epsilon=\alpha+1\geq h$.
This cannot happen because of $t\geq2$, i.e. this maximum point is
over then upper-range limit.

\begin{eqnarray*}
\Rightarrow & max\left\{ q^{i(\alpha t+ht)-i^{2}}:i=1,\ldots,(h-1)t-1\right\}  & =\left.q^{i(\alpha t+ht)-i^{2}}\right|_{i=(h-1)t-1}\\
 &  & =q^{\left[\left(h-1\right)\left(\alpha+1\right)\right]t^{2}-\left(\alpha-h+2\right)t-1}
\end{eqnarray*}

Secondly, we apply the maximum value with the sum, we have:

\[
\stackrel[i=0]{(h-1)t-1}{\mathop{\sum}}p^{(i)}(q)\in\Theta\left(q^{\left[\left(h-1\right)\left(\alpha+1\right)\right]t^{2}+\mathcal{O}(t)}\right)
\]

Thirdly, we consider the 3rd requirement of LLL to figure out a lower
bound on $r_{max}$:

$d\leq\alpha\left(\begin{array}{c}
r-1\\
\alpha-1
\end{array}\right)=\alpha\frac{\left(r-1\right)\ldots\left(r-\alpha+1\right)}{\left(\alpha-1\right)!}\leq\frac{\alpha}{\left(\alpha-1\right)!}r^{^{\alpha-1}}=d_{2}$

We need $4dp\leq1$, which is satisfied if $4d_{2}p\leq1$. Therefore,
we consider:

$\frac{\alpha}{\left(\alpha-1\right)!}r^{^{\alpha-1}}\leq\frac{1}{4p}\Leftrightarrow r\leq\left(\frac{\left(\alpha-1\right)!}{4\alpha}\cdot\frac{1}{p}\right)^{\frac{1}{\alpha-1}}$

Finally, we have from above:

\begin{eqnarray*}
p & \in & \Theta\left(\frac{q^{\left[\left(h-1\right)\left(\alpha+1\right)\right]t^{2}+\mathcal{O}(t)}}{q^{\left(\alpha h\right)t^{2}}}\right)\\
\Rightarrow r_{min,vector} & \in & \Omega\left(q^{\frac{h-\alpha-1}{1-\alpha}t^{2}+\mathcal{O}(t)}\right)
\end{eqnarray*}


\subsection{Find the Upper Bound of $r_{max,scalar}$}

\noindent\fbox{\begin{minipage}[t]{1\columnwidth - 2\fboxsep - 2\fboxrule}%
Find $\left(\alpha+1\right)$ received vectors that span a subspace
of dimension $h$. This implies that the $\alpha$ links from the
middle layer carry $\alpha$ vectors which span a subspace of $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$
whose dimension is at least $\left(h-1\right)$, with $q_{\mathrm{s}}=q^{t}$.%
\end{minipage}}

For $3\leq\alpha<h$: all $\alpha$ links must be distinct $\Rightarrow r\leq\left[\begin{array}{c}
\alpha\\
1
\end{array}\right]_{q_{\mathrm{s}}}\Rightarrow r\leq$

For $\alpha\geq h\geq3$: to achieve $(h-1)$-subspaces of $\ensuremath{\mathbb{F}}_{q_{\mathrm{s}}}^{h}$,
no $\alpha$ links will contain a vector which is contained in the
same $(h-2)$-subspace.

Hence,

\[
r_{max,scalar}\leq\left(\alpha-1\right)\left[\begin{array}{c}
\alpha\\
h-2
\end{array}\right]_{q_{\mathrm{s}}}\Rightarrow r_{max,scalar}\in\mathcal{O}\left(q_{\mathrm{s}}^{\left(\alpha-h+2\right)\left(h-2\right)t^{2}}\right)
\]


\subsection{Calculate Gap }

\begin{eqnarray*}
 & r_{max,scalar} & =r_{min,vector}\\
\Leftrightarrow & q_{\mathrm{s}}^{\left(\alpha-h+2\right)\left(h-2\right)t^{2}} & =q^{\frac{h-\alpha-1}{1-\alpha}t^{2}+\mathcal{O}(t)}\\
\Leftrightarrow & q_{\mathrm{s}} & ^{=}q^{\frac{\alpha-h+1}{\left(\alpha-1\right)\left(\alpha-h+2\right)\left(h-2\right)}t^{2}+\mathcal{O}(t)}\\
\Rightarrow & g & =q_{\mathrm{s}}-q_{v}=q^{\frac{\alpha-h+1}{\left(\alpha-1\right)\left(\alpha-h+2\right)\left(h-2\right)}t^{2}+\mathcal{O}(t)}\Square
\end{eqnarray*}


\part{Computational Results}

In Table \ref{tab:r_over_t}, our vector solutions are computed by
Algorithm \ref{alg:Increasing-Method} for the $\left(\epsilon=1,\ell=1\right)-\ensuremath{N}_{3,r,4}$
network regarding to $t=2$ and $t=3$. Both construction 1 and 2
provide better results than scalar solutions. 

Construction 1: $\begin{array}{c|c}
\boldsymbol{I}_{t} & \boldsymbol{T}\end{array}$, with $\boldsymbol{T}\in\ensuremath{\mathbb{F}}_{q}^{t\times t\left(h-1\right)}$

Construction 2: $\boldsymbol{T}\in MatrixSpaceUrs\left(t,3t\right)$

Regarding to $t=2$, for a scalar network coding solution we need
a $3-\left(3,1,1\right)_{4}^{c}$ code ($q_{v}=2^{2}=4)$ by Theorem
\ref{theo:scalar_sol_exist}. The largest such code consists of the
21 one-dimensionall subspaces of $\ensuremath{\mathbb{F}}_{4}^{3}$,
each one is contained twice in the code. Therefore, the number of
nodes can be at most 42 for a scalar linear coding solution, while
for vector network coding 89 nodes can be used, i..e. $\mathcal{A}_{q=2}\left(n=6,k=4,t=3;\lambda=2\right)\geq89$
following to Corollary \ref{cor:dual_subspaces}. This is a new lower
bound for $\mathcal{A}_{2}\left(6,4,3;2\right)$ compared to a code
with 51 codewords presented in \cite{Wachter-Zeh:2018}. The smallest
alphabet size for a scalar solution with 89 nodes exists is $q_{s}=8$.
By Equation \ref{eq:r_scalar_max}, there are 73 one-dimensional subspaces
of $\ensuremath{\mathbb{F}}_{8}^{3}$, and each one can be used twice
in the code; therefore, we have in total 146 possible codesword, but
only 89 codewords are required. In this case, the gap size $g=q_{s}-q_{v}=2^{3}-2^{2}=8-4=2^{2},$i.e.
we achieve a gap size $q^{t^{2}/2}$, which is better the asymptotic
behavior in \ref{eq:gap_e1l1h3rs4}.
\begin{defn}[Sufficient Global Coding Vector]
 Let $\boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\in\ensuremath{\mathbb{F}}_{q}^{n\times m}$.
Then a set $\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right\} $
forms a subset of $g^{\left(3\right)}$ if

\[
rk\left[\begin{array}{c}
\boldsymbol{A}\\
\boldsymbol{B}\\
\boldsymbol{C}
\end{array}\right]\geq2n
\]

In other words, all $\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right\} $
span a subspace of $\ensuremath{\mathbb{F}}_{q}^{2n}$ whose dimension
is at least $2n$. We denote $g3_{i}$ as a subset of $g3$:

\[
g^{\left(3\right)}=\left\{ \left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right\} _{i}\right\} =\left\{ g_{i}^{\left(3\right)}\right\} ,i=0,1,2,...,\left|g^{\left(3\right)}\right|-1
\]

with $g_{i}^{\left(3\right)}=\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right\} _{i}$
\end{defn}
%
\begin{defn}[Relative]
 Let $\boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\in\ensuremath{\mathbb{F}}_{q}^{n\times m}$.
Then $\boldsymbol{C}$ is called a relative of a tuple $\left(\boldsymbol{A},\boldsymbol{B}\right)$
if $\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right\} \in g^{\left(3\right)}$
and denoted as following:

\[
rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]=\boldsymbol{C}
\]
\end{defn}
%
\begin{defn}[Sub-relative]
 Let $\boldsymbol{A},\boldsymbol{B},\boldsymbol{C},\boldsymbol{D}\in\ensuremath{\mathbb{F}}_{q}^{n\times m}$.
Then $\boldsymbol{D}$ is called a sub-relative of a tuple $\left(\boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right)\in g^{\left(3\right)}$
if:

\[
\left\{ \begin{array}{c}
\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{D}\right\} \in g3\\
\left\{ \boldsymbol{A},\boldsymbol{C},\boldsymbol{D}\right\} \in g3\\
\left\{ \boldsymbol{B},\boldsymbol{C},\boldsymbol{D}\right\} \in g3
\end{array}\right.
\]

It is denoted as: 
\[
subrel\left[\left(\boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right)\right]=\boldsymbol{D}
\]

This definition is reused for a set of 5 or more matrices.
\end{defn}
%
\begin{defn}[MatrixSpace]
 $MatrixSpace(n,m)=\left\{ \boldsymbol{A}:\boldsymbol{A}\in\ensuremath{\mathbb{F}}_{q}^{n\times m}\right\} $
\end{defn}
%
\begin{defn}[MatrixSpace with unique row space]
 $MatrixSpaceUrs(n,m)$ is a subspace of $\ensuremath{\mathbb{F}}_{q}^{n\times m}$,
where any $\boldsymbol{A},\boldsymbol{B}\in\ensuremath{\mathbb{F}}_{q}^{n\times m}$
have their row spaces such that:

\[
\mathcal{R}_{q}\left(\boldsymbol{A}\right)\neq\mathcal{R}_{q}\left(\boldsymbol{B}\right)
\]

where $\mathcal{R}_{q}\left(.\right)$ denotes the row space of a
matrix.
\end{defn}
\begin{algorithm}[H]
\caption{Increasing Method \label{alg:Increasing-Method}}

\textbf{INPUT}: $g^{\left(3\right)}$ of N matrices belonging to $MatrixSpace(n,m)$
or $MatrixSpaceUrs(n,m)$
\begin{enumerate}
\item Create a list of $rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right],\forall\boldsymbol{A},\boldsymbol{B}\in MatrixSpace(n,m),\boldsymbol{A}\neq\boldsymbol{B}$
\item Choose all $\{\boldsymbol{A},\boldsymbol{B}\}$ such that:
\[
\left|rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]\right|=\left|rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]\right|_{max}
\]
with an upper bound for the final result set's cardinality $\left|Res\right|\leq UB,UB=\left|rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]\right|_{max}$.
\item For each found pair set of $\{\boldsymbol{A},\boldsymbol{B}\}$, we
compute the union set of the pair and its Relative, i.e., $\{\boldsymbol{A},\boldsymbol{B}\}\cup rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]$.
If the union set is repeated or duplicated, we take only the first
pair generating such value. We denote the chosen set as $main\_team\_and\_rel$
\item Considering $main\_team\_and\_rel_{i}\in main\_team\_and\_rel$ with
$i=0,1,2,...,\left|main\_team\_and\_rel\right|-1$, we have 
\[
\begin{array}{c}
rel_{j}\in rel\left[main\_team\_and\_rel_{i}\right]\\
\forall main\_team\_and\_rel_{i}\in main\_team\_and\_rel\\
j=0,1,2,...,\left|main\_team\_and\_rel_{i}\right|-1
\end{array}
\]
to compute $n\_main\_team_{i}$, which is combined by $\{\boldsymbol{A},\boldsymbol{B},rel_{j}\}$
if $\left|subrel\left[\left(\boldsymbol{A},\boldsymbol{B},rel_{j}\right)\right]\right|_{max}$
similarly to step 2.
\item Keep only $n\_main\_team_{i}$ with $\left|subrel\left[\left(n\_main\_team_{i}\right)\right]\right|_{max}$
with $i=0,1,2,...,\left|main\_team\_and\_rel\right|-1$. Similar to
step 3, we also avoid duplicated values here.
\item Repeat step 4, 5, 6 until $\left|subrel\left[\left(n\_main\_team_{i}\right)\right]\right|_{max}=0$
\end{enumerate}
\textbf{OUTPUT}: Get the final result set with all matrices such that:

\[
Res=\left\{ \boldsymbol{X}_{i}:\boldsymbol{X}_{i}\in\ensuremath{\mathbb{F}}_{q}^{n\times m}\right\} ,i=0,1,...,UB
\]

with any 3 combinations of $\left(\boldsymbol{X}_{j},\boldsymbol{X}_{k},\boldsymbol{X}_{t}\right)\in g^{\left(3\right)},\forall\boldsymbol{X}_{j},\boldsymbol{X}_{k},\boldsymbol{X}_{t}\in Res,\boldsymbol{X}_{j}\neq\boldsymbol{X}_{k}\neq\boldsymbol{X}_{t}$
and $j\neq k\neq t$.
\end{algorithm}

Example 1: Let $n=1,m=2,q=2$. Then we have $N=4$ matrices (vectors):

\[
\begin{array}{c}
\boldsymbol{A}=[0,0]\\
\boldsymbol{B}=[0,1]\\
\boldsymbol{C}=[1,0]\\
\boldsymbol{D}=[1,1]
\end{array}
\]

\uline{Step 1}: Due to, any 3 of them form a matrix with $rk\geq2n$,
we have the relative as following:

\[
\begin{array}{c}
rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]=[\boldsymbol{C},\boldsymbol{D}]\\
rel\left[\left(\boldsymbol{A},\boldsymbol{C}\right)\right]=[\boldsymbol{B},\boldsymbol{D}]\\
rel\left[\left(\boldsymbol{A},\boldsymbol{D}\right)\right]=[\boldsymbol{B},\boldsymbol{C}]\\
rel\left[\left(\boldsymbol{B},\boldsymbol{C}\right)\right]=[\boldsymbol{A},\boldsymbol{D}]\\
rel\left[\left(\boldsymbol{B},\boldsymbol{D}\right)\right]=[\boldsymbol{A},\boldsymbol{C}]\\
rel\left[\left(\boldsymbol{C},\boldsymbol{D}\right)\right]=[\boldsymbol{A},\boldsymbol{B}]
\end{array}
\]

\uline{Step 2}: We get $UB=2$ and all $\left\{ \boldsymbol{A},\boldsymbol{B}\right\} ,\left\{ \boldsymbol{A},\boldsymbol{C}\right\} ,\left\{ \boldsymbol{A},\boldsymbol{D}\right\} ,\left\{ \boldsymbol{B},\boldsymbol{C}\right\} ,\left\{ \boldsymbol{B},\boldsymbol{D}\right\} ,\left\{ \boldsymbol{C},\boldsymbol{D}\right\} $,
because $\underset{\begin{array}{c}
\forall\boldsymbol{X},\boldsymbol{Y}\in MatrixSpace(1,2)\\
\boldsymbol{X}\neq\boldsymbol{Y}
\end{array}}{max}\left(rel\left[\left(\boldsymbol{X},\boldsymbol{Y}\right)\right]\right)=2$

\uline{Step 3}: Due to $\left\{ \boldsymbol{X},\boldsymbol{Y}\right\} \cup rel\left[\left(\boldsymbol{X},\boldsymbol{Y}\right)\right]=\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C},\boldsymbol{D}\right\} $
with $\left(\boldsymbol{X},\boldsymbol{Y}\right)$ are all tuples
found in Step 2. We keep only $main\_team\_and\_rel=\left\{ \left(\boldsymbol{A},\boldsymbol{B}\right):rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]\right\} $

\uline{Step 4}: Regarding to $rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]$,
we have $rel_{0}=\boldsymbol{C},rel_{1}=\boldsymbol{D}$. Then, $\left|subrel\left[\left(\boldsymbol{A},\boldsymbol{B},rel_{0}\right)\right]\right|=\left|subrel\left[\left(\boldsymbol{A},\boldsymbol{B},rel_{1}\right)\right]\right|=1$,
so we got $n\_main\_team_{0}=\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right\} $
as the only output of this step.

\uline{Step 5}: Because step 4 gets only $\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right\} $,
we do not need to proceed anything here.

\uline{Step 6}: We repeat step 4 and 5 once more and we get $Res=\left\{ \boldsymbol{A},\boldsymbol{B},\boldsymbol{C},\boldsymbol{D}\right\} $,
i.e., all the matrices can be used.

~

Example 2: For further understading, we use Figure \ref{fig:rel_example}
for illustration.

In the right, we observe that the size of relative becomes smaller
when its tuple identity is larger, i.e. $\left|rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]\right|\geq\left|subrel\left[\left(\boldsymbol{A},\boldsymbol{B},\boldsymbol{C}\right)\right]\right|$
or $\left|rel\left[\left(\boldsymbol{A},\boldsymbol{B}\right)\right]\right|\geq\left|subrel\left[\left(\boldsymbol{A},\boldsymbol{B},\boldsymbol{D}\right)\right]\right|$.
It explains why $UB$ is the maximum numbers of matrices that we can
find in $Res$.

Regarding to the left, the visual explanation of $subrel$ is shown.

\begin{figure}[H]
\caption{The vector network coding of $(\epsilon=1,l=1)-\mathcal{N}_{h=3,r,s=4}$
represents as a matrix problem\label{fig:rel_example}}

\centering{}\includegraphics[width=0.5\paperwidth]{E:/Documents/TUM/THESIS/thesisCOD_Ha/figures/rel_example}
\end{figure}

\bibliographystyle{IEEEtran}
\bibliography{../refs/final_ref_bib}

\end{document}
